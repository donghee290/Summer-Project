import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict
import warnings
import logging

# Silence tokenizers fork warning
import os; os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")

from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score, adjusted_rand_score
import hdbscan
import matplotlib.pyplot as plt
from collections import Counter
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

warnings.filterwarnings('ignore')
logging.getLogger('sentence_transformers').setLevel(logging.ERROR)
plt.rcParams['font.family'] = ['AppleGothic', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

def convert_numpy_types(obj):
    """numpy int, float íƒ€ì…ì„ python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜ ì¬ê·€í•¨ìˆ˜"""
    if isinstance(obj, dict):
        return {convert_numpy_types(k): convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(x) for x in obj]
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    else:
        return obj


class KoSimCSENewsPipeline:
    def __init__(self, config: Dict = None):
        self.config = {
            'model_name': 'BM-K/KoSimCSE-roberta-multitask',
            'batch_size': 16,
            'min_text_length': 50,
            'min_title_length': 10,
            'max_text_length': 1000,
            'clustering_methods': ['HDBSCAN', 'K-Means', 'DBSCAN'],
            'hdbscan_params': {
                'min_cluster_size': 20,
                'min_samples': 8,
                'metric': 'euclidean',
                'cluster_selection_epsilon': 0.25
            },
            'kmeans_params': {
                'n_init': 20,
                'max_iter': 500,
                'random_state': 42
            },
            'dbscan_params': {
                'eps': 0.3,
                'min_samples': 10,
                'metric': 'cosine'
            },
            'use_time_bucket': True,
            'time_bucket_hours': 6,
            'use_near_duplicate_merge': True,
            'ndup_vector': 'char',  # 'char' or 'word'
            'ndup_char_ngram_min': 3,
            'ndup_char_ngram_max': 5,
            'ndup_word_ngram_min': 1,
            'ndup_word_ngram_max': 2,
            'ndup_threshold': 0.92,
            'output_dir': str(Path(__file__).resolve().parents[2] / "results" / "cluster_results"),
            'save_visualizations': True
        }
        if config:
            self.config.update(config)

        self.model = None
        self.articles_df = None
        self.embeddings = None
        self.cluster_labels = None
        self.best_method = None
        self.cluster_analysis = None

        base_dir = Path(__file__).resolve().parents[2]
        self.output_dir = (base_dir / "model" / "results" / "cluster_results").resolve()
        self.output_dir.mkdir(parents=True, exist_ok=True)

        print(f"ğŸš€ KoSimCSE ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”")
        print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")

    # ===== Time bucketing & near-duplicate helpers =====
    def _parse_pubdate(self, s: str):
        try:
            dt = pd.to_datetime(s, utc=True, errors='coerce')
            return dt
        except Exception:
            return pd.NaT

    def _add_time_bucket(self):
        if self.articles_df is None or 'pubDate' not in self.articles_df.columns:
            return
        self.articles_df['published_at'] = self.articles_df['pubDate'].apply(self._parse_pubdate)
        hours = int(self.config.get('time_bucket_hours', 6))
        self.articles_df['event_bucket'] = self.articles_df['published_at'].dt.floor(f'{hours}H')

    def _normalize_for_dup(self, text: str) -> str:
        if not isinstance(text, str):
            return ''
        t = text
        t = re.sub(r'[\[\(\{][^\]\)\}]{0,30}[\]\)\}]', ' ', t)
        t = re.sub(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}', ' ', t)
        t = re.sub(r'Â©|ë¬´ë‹¨ì „ì¬|ì¬ë°°í¬|ê¸°ì|ì‚¬ì§„=|ì—°í•©ë‰´ìŠ¤|ë‰´ìŠ¤1', ' ', t)
        t = re.sub(r'\s+', ' ', t).strip().lower()
        return t

    def _merge_near_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        if df.empty:
            return df
        texts = (df['fullText']
                 .fillna('')
                 .apply(self._normalize_for_dup)
                 .tolist())
        vec_mode = str(self.config.get('ndup_vector', 'char'))
        if vec_mode == 'word':
            v = TfidfVectorizer(ngram_range=(int(self.config.get('ndup_word_ngram_min', 1)),
                                             int(self.config.get('ndup_word_ngram_max', 2))),
                                min_df=1, max_df=1.0)
        else:
            v = TfidfVectorizer(analyzer='char',
                                ngram_range=(int(self.config.get('ndup_char_ngram_min', 3)),
                                             int(self.config.get('ndup_char_ngram_max', 5))),
                                min_df=1, max_df=1.0)
        X = v.fit_transform(texts)
        sim = cosine_similarity(X, dense_output=False)
        thr = float(self.config.get('ndup_threshold', 0.92))

        n = df.shape[0]
        adj = [[] for _ in range(n)]
        coo = sim.tocoo()
        for i, j, val in zip(coo.row, coo.col, coo.data):
            if i >= j:
                continue
            if val >= thr:
                adj[i].append(j)
                adj[j].append(i)

        visited = [False] * n
        groups = []
        for i in range(n):
            if visited[i]:
                continue
            stack = [i]
            visited[i] = True
            comp = [i]
            while stack:
                u = stack.pop()
                for vtx in adj[u]:
                    if not visited[vtx]:
                        visited[vtx] = True
                        stack.append(vtx)
                        comp.append(vtx)
            groups.append(comp)

        reps = []
        for comp in groups:
            sub = df.iloc[comp]
            rep_idx = sub['textLength'].astype(float).fillna(0).values.argmax()
            rep_row = sub.iloc[rep_idx].copy()
            rep_row['dup_count'] = int(len(comp))
            rep_row['merged_indices'] = comp
            merged_full = sub.loc[sub['textLength'].astype(float).idxmax(), 'fullText']
            rep_row['fullText_merged'] = merged_full if isinstance(merged_full, str) else rep_row.get('fullText', '')
            reps.append(rep_row)

        merged_df = pd.DataFrame(reps).reset_index(drop=True)
        return merged_df

    def load_model(self) -> bool:
        print(f"\nğŸ¤– ëª¨ë¸ ë¡œë”©: {self.config['model_name']}")
        try:
            self.model = SentenceTransformer(self.config['model_name'])
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!   ì°¨ì›: {self.model.get_sentence_embedding_dimension()}")
            return True
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False

    def load_news_data(self, file_path: str) -> bool:
        print(f"\nğŸ“Š ë‰´ìŠ¤ ë°ì´í„° ë¡œë”©...")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                news_data = json.load(f)
            print(f"âœ… íŒŒì¼ ë¡œë“œ ì„±ê³µ: {file_path}")

            if 'news' not in news_data or not isinstance(news_data['news'], dict):
                print("âŒ JSON êµ¬ì¡°ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. 'news' í‚¤ì™€ ë”•ì…”ë„ˆë¦¬ íƒ€ì…ì„ í™•ì¸í•˜ì„¸ìš”.")
                return False

            articles = []
            category_stats = {}

            for category, article_list in news_data['news'].items():
                if not isinstance(article_list, list):
                    continue
                for article in article_list:
                    title = article.get('title', '').strip()
                    content = article.get('content', '').strip()
                    full_text = f"{title}. {content}".strip()
                    if (len(full_text) >= self.config['min_text_length'] and
                        len(title) >= self.config['min_title_length'] and
                        len(full_text) <= self.config['max_text_length']):
                        articles.append({
                            **article,
                            'title': title,
                            'content': content,
                            'fullText': full_text,
                            'textLength': len(full_text),
                            'category': category,
                            'index': len(articles)
                        })
                        category_stats[category] = category_stats.get(category, 0) + 1

            self.articles_df = pd.DataFrame(articles)
            print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬")
            print("ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
            for cat, count in category_stats.items():
                print(f"  {cat}: {count}ê°œ")

            # === (1) Time bucket enrichment ===
            if self.config.get('use_time_bucket', True):
                self._add_time_bucket()

            # === (2) Near-duplicate merge (super-doc compression) ===
            if self.config.get('use_near_duplicate_merge', True):
                before = len(self.articles_df)
                self.articles_df = self._merge_near_duplicates(self.articles_df)
                after = len(self.articles_df)
                print(f"ğŸ” ê·¼ì ‘ì¤‘ë³µ ë³‘í•©: {before} â†’ {after} ë¬¸ì„œ (ì••ì¶• ë¹„ìœ¨ {(1 - after/max(before,1)):.1%})")

            return True
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return False

    def generate_embeddings(self) -> bool:
        print(f"\nâš¡ KoSimCSE ì„ë² ë”© ìƒì„± ì¤‘...")
        if self.model is None or self.articles_df is None:
            print("âŒ ëª¨ë¸ ë˜ëŠ” ë‰´ìŠ¤ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        try:
            texts = (self.articles_df['fullText_merged']
                     if 'fullText_merged' in self.articles_df.columns
                     else self.articles_df['fullText']).tolist()
            self.embeddings = self.model.encode(
                texts,
                batch_size=self.config['batch_size'],
                show_progress_bar=True,
                convert_to_numpy=True,
                normalize_embeddings=True
            )
            print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ! í˜•íƒœ: {self.embeddings.shape}")
            return True
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return False

    def run_clustering(self) -> Dict:
        print(f"\nğŸ” í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰ ì¤‘...")
        if self.embeddings is None:
            print("âŒ ì„ë² ë”©ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}

        results = {}
        scores = {}

        if 'HDBSCAN' in self.config['clustering_methods']:
            print("1. HDBSCAN í´ëŸ¬ìŠ¤í„°ë§...")
            try:
                clusterer = hdbscan.HDBSCAN(**self.config['hdbscan_params'])
                labels = clusterer.fit_predict(self.embeddings)
                results['HDBSCAN'] = labels
                valid_mask = labels != -1
                if valid_mask.sum() > 1 and len(set(labels[valid_mask])) > 1:
                    score = silhouette_score(self.embeddings[valid_mask], labels[valid_mask])
                    scores['HDBSCAN'] = score
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                n_noise = np.sum(labels == -1)
                print(f"   âœ… ì™„ë£Œ: {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°, ë…¸ì´ì¦ˆ {n_noise}ê°œ")
            except Exception as e:
                print(f"   âŒ HDBSCAN ì‹¤íŒ¨: {e}")

        if 'K-Means' in self.config['clustering_methods']:
            print("2. K-Means í´ëŸ¬ìŠ¤í„°ë§...")
            try:
                n_categories = len(self.articles_df['category'].unique())
                optimal_k = min(n_categories + 2, 15)
                kmeans = KMeans(n_clusters=optimal_k, **self.config['kmeans_params'])
                labels = kmeans.fit_predict(self.embeddings)
                results['K-Means'] = labels
                score = silhouette_score(self.embeddings, labels)
                scores['K-Means'] = score
                print(f"   âœ… ì™„ë£Œ: {optimal_k}ê°œ í´ëŸ¬ìŠ¤í„°, ì‹¤ë£¨ì—£ {score:.3f}")
            except Exception as e:
                print(f"   âŒ K-Means ì‹¤íŒ¨: {e}")

        if 'DBSCAN' in self.config['clustering_methods']:
            print("3. DBSCAN í´ëŸ¬ìŠ¤í„°ë§...")
            try:
                dbscan = DBSCAN(**self.config['dbscan_params'])
                labels = dbscan.fit_predict(self.embeddings)
                results['DBSCAN'] = labels
                valid_mask = labels != -1
                if valid_mask.sum() > 1 and len(set(labels[valid_mask])) > 1:
                    score = silhouette_score(self.embeddings[valid_mask], labels[valid_mask])
                    scores['DBSCAN'] = score
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                n_noise = np.sum(labels == -1)
                print(f"   âœ… ì™„ë£Œ: {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°, ë…¸ì´ì¦ˆ {n_noise}ê°œ")
            except Exception as e:
                print(f"   âŒ DBSCAN ì‹¤íŒ¨: {e}")

        if scores:
            self.best_method = max(scores, key=scores.get)
            self.cluster_labels = results[self.best_method]
            print(f"\nğŸ¯ ìµœì  ë°©ë²•: {self.best_method} (ì‹¤ë£¨ì—£ ê³„ìˆ˜: {scores[self.best_method]:.3f})")
        elif results:
            if 'HDBSCAN' in results:
                self.best_method = 'HDBSCAN'
                self.cluster_labels = results['HDBSCAN']
            else:
                self.best_method = list(results.keys())[0]
                self.cluster_labels = results[self.best_method]
            print(f"\nğŸ¯ ì„ íƒëœ ë°©ë²•: {self.best_method}")

        return results
    
    def analyze_clusters(self) -> Dict:
        print(f"\nğŸ“Š í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì¤‘...")
        if self.cluster_labels is None:
            print("âŒ í´ëŸ¬ìŠ¤í„°ë§ì´ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}

        unique_clusters = sorted(set(self.cluster_labels))
        analysis = {}
        
        for cluster_id in unique_clusters:
            mask = self.cluster_labels == cluster_id
            cluster_articles = self.articles_df[mask]
            
            if cluster_id == -1:  # ë…¸ì´ì¦ˆ
                analysis[cluster_id] = {
                    'size': len(cluster_articles),
                    'type': 'ë…¸ì´ì¦ˆ',
                    'sample_titles': cluster_articles['title'].head(3).tolist()
                }
            else:
                cat_dist = cluster_articles['category'].value_counts()
                dominant_cat = cat_dist.index[0] if len(cat_dist) > 0 else 'Unknown'
                purity = (cat_dist.iloc[0] / len(cluster_articles)) if len(cat_dist) > 0 else 0
                top_articles = cluster_articles.nlargest(3, 'textLength')
                analysis[cluster_id] = {
                    'size': len(cluster_articles),
                    'dominant_category': dominant_cat,
                    'purity': purity,
                    'categories': dict(cat_dist.head(3)),
                    'representative_titles': top_articles['title'].tolist(),
                    'avg_text_length': cluster_articles['textLength'].mean(),
                    'sample_articles': cluster_articles.sample(min(3, len(cluster_articles)))['title'].tolist()
                }
        # numpy íƒ€ì…ì„ json í˜¸í™˜ ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        analysis_conv = {str(int(k)): convert_numpy_types(v) for k, v in analysis.items()}
        self.cluster_analysis = analysis_conv
        return analysis_conv
    
    def calculate_metrics(self) -> Dict:
        print(f"\nğŸ“ í’ˆì§ˆ ì§€í‘œ ê³„ì‚° ì¤‘...")
        if self.cluster_labels is None:
            return {}

        category_to_id = {cat: i for i, cat in enumerate(self.articles_df['category'].unique())}
        true_labels = np.array([category_to_id[cat] for cat in self.articles_df['category']])
        metrics = {}

        valid_mask = self.cluster_labels != -1
        if valid_mask.sum() > 0:
            valid_pred = self.cluster_labels[valid_mask]
            valid_true = true_labels[valid_mask]
            valid_embeddings = self.embeddings[valid_mask]
            if len(set(valid_pred)) > 1:
                silhouette = silhouette_score(valid_embeddings, valid_pred)
                metrics['silhouette'] = silhouette
                ari = adjusted_rand_score(valid_true, valid_pred)
                metrics['ari'] = ari
                total_purity = 0
                for cluster_id in set(valid_pred):
                    cluster_mask = valid_pred == cluster_id
                    if cluster_mask.sum() > 0:
                        cluster_true = valid_true[cluster_mask]
                        most_common_count = Counter(cluster_true).most_common(1)[0][1]
                        cluster_purity = most_common_count / len(cluster_true)
                        total_purity += cluster_purity * len(cluster_true)
                overall_purity = total_purity / len(valid_pred)
                metrics['purity'] = overall_purity
                metrics['n_clusters'] = len(set(valid_pred))
                metrics['noise_ratio'] = (self.cluster_labels == -1).sum() / len(self.cluster_labels)
                print(f"  ì‹¤ë£¨ì—£ ê³„ìˆ˜: {silhouette:.3f}")
                print(f"  ì¡°ì •ëœ ëœë“œ ì§€ìˆ˜: {ari:.3f}")
                print(f"  ì „ì²´ ìˆœë„: {overall_purity:.3f}")
                print(f"  í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(set(valid_pred))}")
                print(f"  ë…¸ì´ì¦ˆ ë¹„ìœ¨: {metrics['noise_ratio']:.1%}")
        return metrics
    
    def save_results(self):
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
        if self.articles_df is None or self.cluster_labels is None:
            print("âŒ ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        try:
            results_df = self.articles_df.copy()
            results_df['cluster'] = self.cluster_labels
            results_df['method'] = self.best_method

            # Attach time bucket & near-duplicate info when available
            if 'event_bucket' in results_df.columns:
                results_df['event_bucket'] = results_df['event_bucket']
            if 'dup_count' in results_df.columns:
                results_df['dup_count'] = results_df['dup_count']
            if 'merged_indices' in results_df.columns:
                results_df['merged_indices'] = results_df['merged_indices']
            if 'fullText_merged' in results_df.columns:
                results_df['fullText_merged'] = results_df['fullText_merged']

            ts = datetime.utcnow().isoformat().replace(":", "-").replace(".", "-")
            csv_path = self.output_dir / f'clustering_results_detailed_{ts}.csv'
            results_df.to_csv(csv_path, index=False, encoding='utf-8')

            if self.cluster_analysis:
                summary_path = self.output_dir / f'cluster_summary_{ts}.json'
                with open(summary_path, 'w', encoding='utf-8') as f:
                    json.dump(self.cluster_analysis, f, ensure_ascii=False, indent=2)
                print(f"   âœ… í´ëŸ¬ìŠ¤í„° ìš”ì•½: {summary_path}")

            # --- Make articles JSON-serializable (convert Timestamp/NaT etc.) ---
            safe_articles_df = self.articles_df.copy()
            def _json_safe(x):
                if isinstance(x, (pd.Timestamp, datetime)):
                    return x.isoformat() if not pd.isna(x) else None
                return x
            safe_articles_df = safe_articles_df.applymap(_json_safe)

            embeddings_data = {
                'articles': safe_articles_df.to_dict('records'),
                'embeddings': self.embeddings.tolist(),
                'cluster_labels': self.cluster_labels.tolist(),
                'metadata': {
                    'model': self.config['model_name'],
                    'method': self.best_method,
                    'dimensions': self.embeddings.shape[1],
                    'total_articles': len(self.articles_df),
                    'timestamp': datetime.now().isoformat()
                }
            }
            embeddings_data = convert_numpy_types(embeddings_data)

            embeddings_path = self.output_dir / f'embeddings_and_clusters_{ts}.json'
            with open(embeddings_path, 'w', encoding='utf-8') as f:
                json.dump(embeddings_data, f, ensure_ascii=False, indent=2)
            print(f"   âœ… ì„ë² ë”© ë°ì´í„°: {embeddings_path}")
        except Exception as e:
            print(f"   âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def run_full_pipeline(self, news_file_path: str) -> bool:
        print("ğŸš€ KoSimCSE ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘\n")
        if not self.load_model():
            return False
        if not self.load_news_data(news_file_path):
            return False
        if not self.generate_embeddings():
            return False
        clustering_results = self.run_clustering()
        if not clustering_results:
            return False
        self.analyze_clusters()
        self.calculate_metrics()
        self.save_results()
        print(f"\nâœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print(f"ğŸ“‚ ê²°ê³¼ ìœ„ì¹˜: {self.output_dir}")
        return True

def main():
    NEWS_DIR = Path(__file__).resolve().parents[2] / "model" / "results" / "collect_results"
    latest = max(NEWS_DIR.glob("news_colletced_*h_*.json"), key=lambda p: p.stat().st_mtime)
    news_file_path = str(latest)
    print("Using:", news_file_path)

    config = {
        'model_name': 'BM-K/KoSimCSE-roberta-multitask',
        'batch_size': 16,
        'min_text_length': 50,
        'output_dir': str(Path(__file__).resolve().parents[2] / "model" / "results" / "cluster_results"),
        'save_visualizations': True,
        'clustering_methods': ['HDBSCAN', 'K-Means', 'DBSCAN']
    }

    pipeline = KoSimCSENewsPipeline(config)
    success = pipeline.run_full_pipeline(news_file_path)

    if success:
        print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
