# kosimcse_full_pipeline.py
import os
import json
import pandas as pd
import numpy as np
import warnings
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional

# ML/í´ëŸ¬ìŠ¤í„°ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.cluster import KMeans, DBSCAN
    from sklearn.metrics import silhouette_score, adjusted_rand_score
    from sklearn.decomposition import PCA
    from sklearn.manifold import TSNE
    import hdbscan
    import matplotlib.pyplot as plt
    import seaborn as sns
    from collections import Counter
    from tqdm import tqdm
except ImportError as e:
    print(f"âŒ í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
    print("pip install sentence-transformers pandas numpy scikit-learn hdbscan matplotlib seaborn tqdm")
    exit(1)

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings('ignore')
logging.getLogger('sentence_transformers').setLevel(logging.ERROR)

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['AppleGothic', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class KoSimCSENewsPipeline:
    """KoSimCSEë¥¼ ì‚¬ìš©í•œ í•œêµ­ì–´ ë‰´ìŠ¤ ì„ë² ë”© ë° í´ëŸ¬ìŠ¤í„°ë§ í†µí•© íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config: Dict = None):
        """
        ì„¤ì • ì´ˆê¸°í™”
        
        Args:
            config: íŒŒì´í”„ë¼ì¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        # ê¸°ë³¸ ì„¤ì •
        self.config = {
            'model_name': 'BM-K/KoSimCSE-roberta-multitask',
            'batch_size': 32,
            'min_text_length': 100,
            'min_title_length': 10,
            'max_text_length': 1000,
            'clustering_methods': ['HDBSCAN', 'K-Means', 'DBSCAN'],
            'hdbscan_params': {
                'min_cluster_size': 20,
                'min_samples': 8,
                'metric': 'cosine',
                'cluster_selection_epsilon': 0.25
            },
            'kmeans_params': {
                'n_init': 20,
                'max_iter': 500,
                'random_state': 42
            },
            'dbscan_params': {
                'eps': 0.3,
                'min_samples': 10,
                'metric': 'cosine'
            },
            'output_dir': 'kosimcse_results',
            'save_visualizations': True
        }
        
        # ì‚¬ìš©ì ì„¤ì •ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        if config:
            self.config.update(config)
        
        # ìƒíƒœ ë³€ìˆ˜
        self.model = None
        self.articles_df = None
        self.embeddings = None
        self.cluster_labels = None
        self.best_method = None
        self.cluster_analysis = None
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir = Path(self.config['output_dir'])
        self.output_dir.mkdir(exist_ok=True)
        
        print(f"ğŸš€ KoSimCSE ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”")
        print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
    
    def check_dependencies(self) -> bool:
        """í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸"""
        required_packages = [
            'sentence_transformers', 'sklearn', 'hdbscan', 
            'matplotlib', 'seaborn', 'pandas', 'numpy'
        ]
        
        missing_packages = []
        for package in required_packages:
            try:
                __import__(package)
            except ImportError:
                missing_packages.append(package)
        
        if missing_packages:
            print(f"âŒ ë‹¤ìŒ íŒ¨í‚¤ì§€ë“¤ì´ í•„ìš”í•©ë‹ˆë‹¤: {missing_packages}")
            print("ì„¤ì¹˜ ëª…ë ¹ì–´:")
            print("pip install sentence-transformers pandas numpy scikit-learn hdbscan matplotlib seaborn tqdm")
            return False
        
        print("âœ… ëª¨ë“  í•„ìš” íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
        return True
    
    def load_model(self) -> bool:
        """KoSimCSE ëª¨ë¸ ë¡œë“œ"""
        print(f"\nğŸ¤– ëª¨ë¸ ë¡œë”©: {self.config['model_name']}")
        
        try:
            self.model = SentenceTransformer(self.config['model_name'])
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
            print(f"   ì°¨ì›: {self.model.get_sentence_embedding_dimension()}")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def find_news_file(self) -> Optional[str]:
        """ë‰´ìŠ¤ JSON íŒŒì¼ ìë™ íƒì§€"""
        possible_locations = [
            'source/',
            './',
            '../',
            'data/'
        ]
        
        file_patterns = [
            'v3_naver_news_cleaned_1hour_2025-07-29T02-19-57-117Z.json',
            '*naver_news*.json',
            '*news*.json'
        ]
        
        for location in possible_locations:
            location_path = Path(location)
            if location_path.exists():
                for pattern in file_patterns:
                    files = list(location_path.glob(pattern))
                    if files:
                        return str(files[0])
        
        return None
    
    def load_news_data(self, file_path: str = None) -> bool:
        """ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print(f"\nğŸ“Š ë‰´ìŠ¤ ë°ì´í„° ë¡œë”©...")
        
        # íŒŒì¼ ê²½ë¡œ ìë™ íƒì§€
        if file_path is None:
            file_path = self.find_news_file()
            if file_path is None:
                print("âŒ ë‰´ìŠ¤ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ í™•ì¸í•˜ì„¸ìš”:")
                print("- source/v3_naver_news_cleaned_1hour_2025-07-29T02-19-57-117Z.json")
                print("- ./ë‰´ìŠ¤íŒŒì¼.json")
                return False
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                news_data = json.load(f)
            
            print(f"âœ… íŒŒì¼ ë¡œë“œ ì„±ê³µ: {file_path}")
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            articles = []
            category_stats = {}
            
            for category, article_list in news_data['news'].items():
                if not isinstance(article_list, list):
                    continue
                
                for article in article_list:
                    title = article.get('title', '').strip()
                    content = article.get('content', '').strip()
                    full_text = f"{title}. {content}".strip()
                    
                    # í’ˆì§ˆ í•„í„°ë§
                    if (len(full_text) >= self.config['min_text_length'] and 
                        len(title) >= self.config['min_title_length'] and
                        len(full_text) <= self.config['max_text_length']):
                        
                        articles.append({
                            **article,
                            'title': title,
                            'content': content,
                            'fullText': full_text,
                            'textLength': len(full_text),
                            'category': category,
                            'index': len(articles)
                        })
                        
                        category_stats[category] = category_stats.get(category, 0) + 1
            
            # DataFrame ìƒì„±
            self.articles_df = pd.DataFrame(articles)
            
            print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬")
            print("ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
            for cat, count in category_stats.items():
                print(f"   {cat}: {count}ê°œ")
            
            return True
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def generate_embeddings(self) -> bool:
        """KoSimCSE ì„ë² ë”© ìƒì„±"""
        print(f"\nâš¡ KoSimCSE ì„ë² ë”© ìƒì„± ì¤‘...")
        
        if self.model is None:
            print("âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        if self.articles_df is None:
            print("âŒ ë‰´ìŠ¤ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            texts = self.articles_df['fullText'].tolist()
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„ë² ë”© ìƒì„±
            self.embeddings = self.model.encode(
                texts,
                batch_size=self.config['batch_size'],
                show_progress_bar=True,
                convert_to_numpy=True,
                normalize_embeddings=True  # SimCSE í‘œì¤€
            )
            
            print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ! í˜•íƒœ: {self.embeddings.shape}")
            return True
            
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def run_clustering(self) -> Dict:
        """ë‹¤ì–‘í•œ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰ ë° í‰ê°€"""
        print(f"\nğŸ” í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰ ì¤‘...")
        
        if self.embeddings is None:
            print("âŒ ì„ë² ë”©ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}
        
        results = {}
        scores = {}
        
        # 1. HDBSCAN
        if 'HDBSCAN' in self.config['clustering_methods']:
            print("1. HDBSCAN í´ëŸ¬ìŠ¤í„°ë§...")
            try:
                clusterer = hdbscan.HDBSCAN(**self.config['hdbscan_params'])
                labels = clusterer.fit_predict(self.embeddings)
                results['HDBSCAN'] = labels
                
                # í‰ê°€
                valid_mask = labels != -1
                if valid_mask.sum() > 1 and len(set(labels[valid_mask])) > 1:
                    score = silhouette_score(self.embeddings[valid_mask], labels[valid_mask])
                    scores['HDBSCAN'] = score
                
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                n_noise = np.sum(labels == -1)
                print(f"   âœ… ì™„ë£Œ: {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°, ë…¸ì´ì¦ˆ {n_noise}ê°œ")
                
            except Exception as e:
                print(f"   âŒ HDBSCAN ì‹¤íŒ¨: {e}")
        
        # 2. K-Means
        if 'K-Means' in self.config['clustering_methods']:
            print("2. K-Means í´ëŸ¬ìŠ¤í„°ë§...")
            try:
                n_categories = len(self.articles_df['category'].unique())
                optimal_k = min(n_categories + 2, 15)
                
                kmeans = KMeans(n_clusters=optimal_k, **self.config['kmeans_params'])
                labels = kmeans.fit_predict(self.embeddings)
                results['K-Means'] = labels
                
                # í‰ê°€
                score = silhouette_score(self.embeddings, labels)
                scores['K-Means'] = score
                
                print(f"   âœ… ì™„ë£Œ: {optimal_k}ê°œ í´ëŸ¬ìŠ¤í„°, ì‹¤ë£¨ì—£ {score:.3f}")
                
            except Exception as e:
                print(f"   âŒ K-Means ì‹¤íŒ¨: {e}")
        
        # 3. DBSCAN
        if 'DBSCAN' in self.config['clustering_methods']:
            print("3. DBSCAN í´ëŸ¬ìŠ¤í„°ë§...")
            try:
                dbscan = DBSCAN(**self.config['dbscan_params'])
                labels = dbscan.fit_predict(self.embeddings)
                results['DBSCAN'] = labels
                
                # í‰ê°€
                valid_mask = labels != -1
                if valid_mask.sum() > 1 and len(set(labels[valid_mask])) > 1:
                    score = silhouette_score(self.embeddings[valid_mask], labels[valid_mask])
                    scores['DBSCAN'] = score
                
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                n_noise = np.sum(labels == -1)
                print(f"   âœ… ì™„ë£Œ: {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°, ë…¸ì´ì¦ˆ {n_noise}ê°œ")
                
            except Exception as e:
                print(f"   âŒ DBSCAN ì‹¤íŒ¨: {e}")
        
        # ìµœì  ë°©ë²• ì„ íƒ
        if scores:
            self.best_method = max(scores, key=scores.get)
            self.cluster_labels = results[self.best_method]
            print(f"\nğŸ¯ ìµœì  ë°©ë²•: {self.best_method} (ì‹¤ë£¨ì—£ ê³„ìˆ˜: {scores[self.best_method]:.3f})")
        elif results:
            # ìŠ¤ì½”ì–´ê°€ ì—†ìœ¼ë©´ HDBSCAN ìš°ì„ 
            if 'HDBSCAN' in results:
                self.best_method = 'HDBSCAN'
                self.cluster_labels = results['HDBSCAN']
            else:
                self.best_method = list(results.keys())[0]
                self.cluster_labels = results[self.best_method]
            print(f"\nğŸ¯ ì„ íƒëœ ë°©ë²•: {self.best_method}")
        
        return results
    
    def analyze_clusters(self) -> Dict:
        """í´ëŸ¬ìŠ¤í„° ìƒì„¸ ë¶„ì„"""
        print(f"\nğŸ“Š í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì¤‘...")
        
        if self.cluster_labels is None:
            print("âŒ í´ëŸ¬ìŠ¤í„°ë§ì´ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}
        
        unique_clusters = sorted(set(self.cluster_labels))
        analysis = {}
        
        for cluster_id in unique_clusters:
            mask = self.cluster_labels == cluster_id
            cluster_articles = self.articles_df[mask]
            
            if cluster_id == -1:  # ë…¸ì´ì¦ˆ
                analysis[cluster_id] = {
                    'size': len(cluster_articles),
                    'type': 'ë…¸ì´ì¦ˆ',
                    'sample_titles': cluster_articles['title'].head(3).tolist()
                }
            else:
                # ì¹´í…Œê³ ë¦¬ ë¶„í¬
                cat_dist = cluster_articles['category'].value_counts()
                dominant_cat = cat_dist.index[0] if len(cat_dist) > 0 else 'Unknown'
                purity = (cat_dist.iloc[0] / len(cluster_articles)) if len(cat_dist) > 0 else 0
                
                # ëŒ€í‘œ ê¸°ì‚¬ë“¤
                top_articles = cluster_articles.nlargest(3, 'textLength')
                
                analysis[cluster_id] = {
                    'size': len(cluster_articles),
                    'dominant_category': dominant_cat,
                    'purity': purity,
                    'categories': dict(cat_dist.head(3)),
                    'representative_titles': top_articles['title'].tolist(),
                    'avg_text_length': cluster_articles['textLength'].mean(),
                    'sample_articles': cluster_articles.sample(min(3, len(cluster_articles)))['title'].tolist()
                }
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"ì´ {len(unique_clusters)}ê°œ í´ëŸ¬ìŠ¤í„°:")
        for cid, info in analysis.items():
            if cid == -1:
                print(f"  ë…¸ì´ì¦ˆ: {info['size']}ê°œ")
            else:
                print(f"\n  í´ëŸ¬ìŠ¤í„° {cid}: {info['size']}ê°œ ê¸°ì‚¬")
                print(f"    ì£¼ë„ ì¹´í…Œê³ ë¦¬: {info['dominant_category']} (ìˆœë„: {info['purity']:.2f})")
                print(f"    ëŒ€í‘œ ì œëª©: {info['representative_titles'][0][:50]}...")
        
        self.cluster_analysis = analysis
        return analysis
    
    def calculate_metrics(self) -> Dict:
        """í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ ì§€í‘œ ê³„ì‚°"""
        print(f"\nğŸ“ í’ˆì§ˆ ì§€í‘œ ê³„ì‚° ì¤‘...")
        
        if self.cluster_labels is None:
            return {}
        
        # ì‹¤ì œ ì¹´í…Œê³ ë¦¬ë¥¼ ìˆ«ìë¡œ ë³€í™˜
        category_to_id = {cat: i for i, cat in enumerate(self.articles_df['category'].unique())}
        true_labels = np.array([category_to_id[cat] for cat in self.articles_df['category']])
        
        metrics = {}
        
        # ë…¸ì´ì¦ˆ ì œì™¸í•œ í‰ê°€
        valid_mask = self.cluster_labels != -1
        if valid_mask.sum() > 0:
            valid_pred = self.cluster_labels[valid_mask]
            valid_true = true_labels[valid_mask]
            valid_embeddings = self.embeddings[valid_mask]
            
            if len(set(valid_pred)) > 1:
                # ì‹¤ë£¨ì—£ ê³„ìˆ˜
                silhouette = silhouette_score(valid_embeddings, valid_pred)
                metrics['silhouette'] = silhouette
                
                # ì¡°ì •ëœ ëœë“œ ì§€ìˆ˜
                ari = adjusted_rand_score(valid_true, valid_pred)
                metrics['ari'] = ari
                
                # ìˆœë„ ê³„ì‚°
                total_purity = 0
                for cluster_id in set(valid_pred):
                    cluster_mask = valid_pred == cluster_id
                    if cluster_mask.sum() > 0:
                        cluster_true = valid_true[cluster_mask]
                        most_common_count = Counter(cluster_true).most_common(1)[0][1]
                        cluster_purity = most_common_count / len(cluster_true)
                        total_purity += cluster_purity * len(cluster_true)
                
                overall_purity = total_purity / len(valid_pred)
                metrics['purity'] = overall_purity
                
                # ê¸°íƒ€ ì§€í‘œ
                metrics['n_clusters'] = len(set(valid_pred))
                metrics['noise_ratio'] = (self.cluster_labels == -1).sum() / len(self.cluster_labels)
                
                # ê²°ê³¼ ì¶œë ¥
                print(f"  ì‹¤ë£¨ì—£ ê³„ìˆ˜: {silhouette:.3f}")
                print(f"  ì¡°ì •ëœ ëœë“œ ì§€ìˆ˜: {ari:.3f}")
                print(f"  ì „ì²´ ìˆœë„: {overall_purity:.3f}")
                print(f"  í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(set(valid_pred))}")
                print(f"  ë…¸ì´ì¦ˆ ë¹„ìœ¨: {metrics['noise_ratio']:.1%}")
        
        return metrics
    
    def create_visualizations(self):
        """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™”"""
        if not self.config['save_visualizations'] or self.embeddings is None:
            return
        
        print(f"\nğŸ¨ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        try:
            # t-SNE ì°¨ì› ì¶•ì†Œ
            print("   t-SNE ì°¨ì› ì¶•ì†Œ...")
            tsne = TSNE(n_components=2, random_state=42, perplexity=30)
            embeddings_2d = tsne.fit_transform(self.embeddings)
            
            # í”Œë¡¯ ìƒì„±
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            
            # 1. í´ëŸ¬ìŠ¤í„°ë³„ ìƒ‰ìƒ
            if self.cluster_labels is not None:
                scatter1 = axes[0, 0].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], 
                                            c=self.cluster_labels, cmap='tab20', alpha=0.7, s=30)
                axes[0, 0].set_title(f'{self.best_method} í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼')
                axes[0, 0].set_xlabel('t-SNE 1')
                axes[0, 0].set_ylabel('t-SNE 2')
            
            # 2. ì‹¤ì œ ì¹´í…Œê³ ë¦¬ë³„
            category_ids = pd.Categorical(self.articles_df['category']).codes
            scatter2 = axes[0, 1].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], 
                                        c=category_ids, cmap='Set3', alpha=0.7, s=30)
            axes[0, 1].set_title('ì‹¤ì œ ì¹´í…Œê³ ë¦¬')
            axes[0, 1].set_xlabel('t-SNE 1')
            axes[0, 1].set_ylabel('t-SNE 2')
            
            # 3. í…ìŠ¤íŠ¸ ê¸¸ì´ë³„
            text_lengths = self.articles_df['textLength'].values
            scatter3 = axes[1, 0].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], 
                                        c=text_lengths, cmap='viridis', alpha=0.7, s=30)
            axes[1, 0].set_title('í…ìŠ¤íŠ¸ ê¸¸ì´ë³„')
            axes[1, 0].set_xlabel('t-SNE 1')
            axes[1, 0].set_ylabel('t-SNE 2')
            plt.colorbar(scatter3, ax=axes[1, 0])
            
            # 4. í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„í¬
            if self.cluster_labels is not None:
                cluster_sizes = pd.Series(self.cluster_labels).value_counts().sort_index()
                axes[1, 1].bar(range(len(cluster_sizes)), cluster_sizes.values)
                axes[1, 1].set_title('í´ëŸ¬ìŠ¤í„°ë³„ ê¸°ì‚¬ ìˆ˜')
                axes[1, 1].set_xlabel('í´ëŸ¬ìŠ¤í„° ID')
                axes[1, 1].set_ylabel('ê¸°ì‚¬ ìˆ˜')
            
            plt.tight_layout()
            
            # ì €ì¥
            viz_path = self.output_dir / 'clustering_visualization.png'
            plt.savefig(viz_path, dpi=300, bbox_inches='tight')
            print(f"   âœ… ì‹œê°í™” ì €ì¥: {viz_path}")
            
            plt.close()
            
        except Exception as e:
            print(f"   âŒ ì‹œê°í™” ì‹¤íŒ¨: {e}")
    
    def save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        if self.articles_df is None or self.cluster_labels is None:
            print("âŒ ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # 1. ìƒì„¸ ê²°ê³¼ CSV
            results_df = self.articles_df.copy()
            results_df['cluster'] = self.cluster_labels
            results_df['method'] = self.best_method
            
            csv_path = self.output_dir / 'clustering_results_detailed.csv'
            results_df.to_csv(csv_path, index=False, encoding='utf-8')
            print(f"   âœ… ìƒì„¸ ê²°ê³¼: {csv_path}")
            
            # 2. í´ëŸ¬ìŠ¤í„° ìš”ì•½ JSON
            if self.cluster_analysis:
                summary_path = self.output_dir / 'cluster_summary.json'
                with open(summary_path, 'w', encoding='utf-8') as f:
                    json.dump(self.cluster_analysis, f, ensure_ascii=False, indent=2)
                print(f"   âœ… í´ëŸ¬ìŠ¤í„° ìš”ì•½: {summary_path}")
            
            # 3. ì„ë² ë”© ë°ì´í„°
            embeddings_data = {
                'articles': self.articles_df.to_dict('records'),
                'embeddings': self.embeddings.tolist(),
                'cluster_labels': self.cluster_labels.tolist(),
                'metadata': {
                    'model': self.config['model_name'],
                    'method': self.best_method,
                    'dimensions': self.embeddings.shape[1],
                    'total_articles': len(self.articles_df),
                    'timestamp': datetime.now().isoformat()
                }
            }
            
            embeddings_path = self.output_dir / 'embeddings_and_clusters.json'
            with open(embeddings_path, 'w', encoding='utf-8') as f:
                json.dump(embeddings_data, f, ensure_ascii=False, indent=2)
            print(f"   âœ… ì„ë² ë”© ë°ì´í„°: {embeddings_path}")
            
        except Exception as e:
            print(f"   âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def run_similarity_test(self):
        """ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ§ª ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸...")
        
        if self.model is None or self.embeddings is None:
            return
        
        test_queries = [
            'êµ­ë‚´ ê²½ì œ',
            'ì‚¬íšŒ',
            'ì—°ì˜ˆ',
            'êµ­ì œ ê²½ì œ'
        ]
        
        for query in test_queries:
            print(f"\n'{query}' ê´€ë ¨ ê¸°ì‚¬ Top 3:")
            
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.model.encode([query], normalize_embeddings=True)[0]
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarities = np.dot(self.embeddings, query_embedding)
            top_indices = np.argsort(similarities)[-3:][::-1]
            
            for i, idx in enumerate(top_indices):
                title = self.articles_df.iloc[idx]['title']
                category = self.articles_df.iloc[idx]['category']
                print(f"  {i+1}. [{similarities[idx]:.4f}] {title[:50]}... ({category})")
    
    def run_full_pipeline(self, news_file_path: str = None) -> bool:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("ğŸš€ KoSimCSE ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘\n")
        
        # 1. ì˜ì¡´ì„± í™•ì¸
        if not self.check_dependencies():
            return False
        
        # 2. ëª¨ë¸ ë¡œë“œ
        if not self.load_model():
            return False
        
        # 3. ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ
        if not self.load_news_data(news_file_path):
            return False
        
        # 4. ì„ë² ë”© ìƒì„±
        if not self.generate_embeddings():
            return False
        
        # 5. í´ëŸ¬ìŠ¤í„°ë§
        clustering_results = self.run_clustering()
        if not clustering_results:
            return False
        
        # 6. í´ëŸ¬ìŠ¤í„° ë¶„ì„
        self.analyze_clusters()
        
        # 7. í’ˆì§ˆ í‰ê°€
        metrics = self.calculate_metrics()
        
        # 8. ì‹œê°í™”
        self.create_visualizations()
        
        # 9. ê²°ê³¼ ì €ì¥
        self.save_results()
        
        # 10. ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸
        self.run_similarity_test()
        
        print(f"\nâœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print(f"ğŸ“‚ ê²°ê³¼ ìœ„ì¹˜: {self.output_dir}")
        
        if metrics:
            print(f"\nğŸ¯ ìµœì¢… ì„±ëŠ¥:")
            print(f"   ë°©ë²•: {self.best_method}")
            if 'silhouette' in metrics:
                print(f"   ì‹¤ë£¨ì—£ ê³„ìˆ˜: {metrics['silhouette']:.3f}")
            if 'purity' in metrics:
                print(f"   ìˆœë„: {metrics['purity']:.3f}")
            if 'n_clusters' in metrics:
                print(f"   í´ëŸ¬ìŠ¤í„° ìˆ˜: {metrics['n_clusters']}")
        
        return True

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    # ëª…í™•í•˜ê²Œ ì‚¬ìš©í•  íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •
    news_file_path = './sample_results/v3_naver_news_cleaned_1hour_2025-08-05T01-50-18-041Z.json'

    config = {
        'model_name': 'BM-K/KoSimCSE-roberta-multitask',
        'batch_size': 16,
        'min_text_length': 50,
        'output_dir': 'kosimcse_results',
        'save_visualizations': True,
        'clustering_methods': ['HDBSCAN', 'K-Means', 'DBSCAN']
    }

    pipeline = KoSimCSENewsPipeline(config)
    # ì—¬ê¸°ì„œ news_file_path ì¸ìë¥¼ ëª…í™•í•˜ê²Œ ë„˜ê¹ë‹ˆë‹¤.
    success = pipeline.run_full_pipeline(news_file_path)

    if success:
        print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()

