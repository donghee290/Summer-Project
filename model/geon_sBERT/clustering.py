# clustering.py
# Pythonì—ì„œ HDBSCANì„ ì‚¬ìš©í•œ ë‰´ìŠ¤ ê¸°ì‚¬ í´ëŸ¬ìŠ¤í„°ë§

import pandas as pd
import numpy as np
import json
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# HDBSCAN import with error handling
try:
    from sklearn.cluster import HDBSCAN
    print("âœ“ sklearnì˜ HDBSCANì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
except ImportError:
    try:
        import hdbscan
        HDBSCAN = hdbscan.HDBSCAN
        print("âœ“ hdbscan íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    except ImportError:
        print("âŒ HDBSCANì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
        print("  pip install scikit-learn>=1.3.0  (ê¶Œì¥)")
        print("  pip install hdbscan")
        exit(1)

warnings.filterwarnings('ignore')

def load_embeddings_from_nodejs():
    """Node.jsì—ì„œ ìƒì„±í•œ ì„ë² ë”© ë°ì´í„° ë¡œë“œ"""
    
    print("=== 1ë‹¨ê³„: ì„ë² ë”© ë°ì´í„° ë¡œë“œ ===")
    
    # ë°©ë²• 1: CSV íŒŒì¼ì—ì„œ ë¡œë“œ
    try:
        print("CSV íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ ì‹œë„...")
        df = pd.read_csv('embeddings_for_clustering.csv')
        
        print(f"  - ê¸°ì‚¬ ê°œìˆ˜: {len(df)}ê°œ")
        print(f"  - ì»¬ëŸ¼: {list(df.columns)}")
        
        # ì„ë² ë”© ë¬¸ìì—´ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
        print("  - ì„ë² ë”© ë²¡í„° ë³€í™˜ ì¤‘...")
        embeddings = []
        
        for i, embedding_str in enumerate(df['embedding']):
            if i % 100 == 0:
                print(f"    ì§„í–‰ë¥ : {i+1}/{len(df)}")
            
            try:
                embedding = [float(x) for x in embedding_str.split(',')]
                embeddings.append(embedding)
            except:
                print(f"    ê²½ê³ : {i}ë²ˆì§¸ ì„ë² ë”© ë³€í™˜ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
                continue
        
        embeddings = np.array(embeddings)
        
        print(f"âœ“ CSVì—ì„œ ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ê¸°ì‚¬, {embeddings.shape[1]}ì°¨ì› ì„ë² ë”©")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬ ì¶œë ¥
        if 'category' in df.columns:
            print("  ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
            category_counts = df['category'].value_counts()
            for cat, count in category_counts.items():
                print(f"    - {cat}: {count}ê°œ")
        
        return df, embeddings
    
    except FileNotFoundError:
        print("âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ CSV ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ë°©ë²• 2: JSON íŒŒì¼ì—ì„œ ì§ì ‘ ë¡œë“œ
    try:
        print("JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ ì‹œë„...")
        with open('embeddings_array.json', 'r') as f:
            embeddings = np.array(json.load(f))
        
        print(f"âœ“ JSONì—ì„œ ë¡œë“œ ì™„ë£Œ: {embeddings.shape[0]}ê°œ ê¸°ì‚¬, {embeddings.shape[1]}ì°¨ì› ì„ë² ë”©")
        print("âš ï¸  ê¸°ì‚¬ ì •ë³´ê°€ ì—†ì–´ í´ëŸ¬ìŠ¤í„° ë¶„ì„ì´ ì œí•œë©ë‹ˆë‹¤.")
        return None, embeddings
    
    except FileNotFoundError:
        print("âŒ JSON íŒŒì¼ë„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ JSON ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ì•ˆë‚´
    print("\níŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
    print("1. Node.js ìŠ¤í¬ë¦½íŠ¸(embedding_script.js)ë¥¼ ë¨¼ì € ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸")
    print("2. í˜„ì¬ ë””ë ‰í† ë¦¬ì— ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸:")
    print("   - embeddings_for_clustering.csv")
    print("   - embeddings_array.json")
    
    import os
    files = [f for f in os.listdir('.') if f.endswith(('.csv', '.json'))]
    if files:
        print("\ní˜„ì¬ ë””ë ‰í† ë¦¬ì˜ ê´€ë ¨ íŒŒì¼ë“¤:")
        for file in files:
            print(f"   - {file}")
    
    raise FileNotFoundError("ì„ë² ë”© íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def perform_hdbscan_clustering(embeddings, min_cluster_size=3, min_samples=2):
    """HDBSCANì„ ì‚¬ìš©í•œ í´ëŸ¬ìŠ¤í„°ë§"""
    
    print(f"\n=== 2ë‹¨ê³„: HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ===")
    print(f"íŒŒë¼ë¯¸í„°:")
    print(f"  - ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸°: {min_cluster_size}")
    print(f"  - ìµœì†Œ ìƒ˜í”Œ ìˆ˜: {min_samples}")
    print(f"  - ê±°ë¦¬ ì¸¡ì •: euclidean")
    
    print("HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰ ì¤‘...")
    
    # HDBSCAN í´ëŸ¬ìŠ¤í„°ë§
    clusterer = HDBSCAN(
        min_cluster_size=min_cluster_size,  # ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸°
        min_samples=min_samples,           # ìµœì†Œ ìƒ˜í”Œ ìˆ˜  
        metric='euclidean',                # ê±°ë¦¬ ì¸¡ì • ë°©ì‹
        cluster_selection_method='eom'     # í´ëŸ¬ìŠ¤í„° ì„ íƒ ë°©ë²•
    )
    
    cluster_labels = clusterer.fit_predict(embeddings)
    
    # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë¶„ì„
    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
    n_noise = list(cluster_labels).count(-1)
    n_clustered = len(cluster_labels) - n_noise
    
    print(f"\ní´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:")
    print(f"  - ë°œê²¬ëœ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜: {n_clusters}ê°œ")
    print(f"  - í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ê¸°ì‚¬: {n_clustered}ê°œ ({n_clustered/len(cluster_labels)*100:.1f}%)")
    print(f"  - ë…¸ì´ì¦ˆë¡œ ë¶„ë¥˜ëœ ê¸°ì‚¬: {n_noise}ê°œ ({n_noise/len(cluster_labels)*100:.1f}%)")
    
    # ê° í´ëŸ¬ìŠ¤í„°ë³„ í¬ê¸°
    if n_clusters > 0:
        print(f"\ní´ëŸ¬ìŠ¤í„°ë³„ í¬ê¸°:")
        cluster_sizes = {}
        for label in cluster_labels:
            if label >= 0:
                cluster_sizes[label] = cluster_sizes.get(label, 0) + 1
        
        for cluster_id in sorted(cluster_sizes.keys()):
            print(f"  - í´ëŸ¬ìŠ¤í„° {cluster_id}: {cluster_sizes[cluster_id]}ê°œ ê¸°ì‚¬")
    
    return cluster_labels, clusterer

def visualize_clusters(embeddings, cluster_labels, method='tsne'):
    """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ 2Dë¡œ ì‹œê°í™”"""
    
    print(f"\n=== 3ë‹¨ê³„: í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™” ({method.upper()}) ===")
    
    if method == 'pca':
        print("PCAë¥¼ ì‚¬ìš©í•œ ì°¨ì› ì¶•ì†Œ...")
        reducer = PCA(n_components=2, random_state=42)
    else:  # tsne
        print("t-SNEë¥¼ ì‚¬ìš©í•œ ì°¨ì› ì¶•ì†Œ...")
        print("  ì£¼ì˜: t-SNEëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
        reducer = TSNE(n_components=2, random_state=42, perplexity=30)
    
    # ì°¨ì› ì¶•ì†Œ
    embeddings_2d = reducer.fit_transform(embeddings)
    print("âœ“ ì°¨ì› ì¶•ì†Œ ì™„ë£Œ!")
    
    # ì‹œê°í™”
    print("ì‹œê°í™” ìƒì„± ì¤‘...")
    plt.figure(figsize=(12, 8))
    plt.rcParams['font.family'] = ['DejaVu Sans', 'Malgun Gothic', 'AppleGothic']  # í•œê¸€ í°íŠ¸ ì„¤ì •
    
    # ê° í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ë‹¤ë¥¸ ìƒ‰ìƒìœ¼ë¡œ í‘œì‹œ
    unique_labels = set(cluster_labels)
    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
    
    if n_clusters > 0:
        colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))
    else:
        colors = ['black']
    
    for k, col in zip(unique_labels, colors):
        if k == -1:
            # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ëŠ” ê²€ì€ìƒ‰ìœ¼ë¡œ
            col = [0, 0, 0, 1]
            marker = 'x'
            label = 'ë…¸ì´ì¦ˆ'
        else:
            marker = 'o'
            label = f'í´ëŸ¬ìŠ¤í„° {k}'
        
        class_member_mask = (cluster_labels == k)
        xy = embeddings_2d[class_member_mask]
        
        if len(xy) > 0:
            plt.scatter(xy[:, 0], xy[:, 1], c=[col], marker=marker, 
                       s=50, alpha=0.7, label=label)
    
    plt.title(f'ë‰´ìŠ¤ ê¸°ì‚¬ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ({method.upper()})', fontsize=14, fontweight='bold')
    plt.xlabel('ì²« ë²ˆì§¸ ì°¨ì›', fontsize=12)
    plt.ylabel('ë‘ ë²ˆì§¸ ì°¨ì›', fontsize=12)
    
    if n_clusters > 0:
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    plt.tight_layout()
    
    # íŒŒì¼ ì €ì¥
    filename = f'news_clustering_{method}.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"âœ“ ì‹œê°í™” ì €ì¥: {filename}")
    
    plt.show()
    
    return embeddings_2d

def analyze_clusters(df, cluster_labels):
    """ê° í´ëŸ¬ìŠ¤í„°ì˜ íŠ¹ì„± ë¶„ì„"""
    
    print(f"\n=== 4ë‹¨ê³„: í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë¶„ì„ ===")
    
    if df is None:
        print("âŒ ê¸°ì‚¬ ì •ë³´ê°€ ì—†ì–´ í´ëŸ¬ìŠ¤í„° ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("CSV íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ë©´ ë” ìì„¸í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        return None
    
    # í´ëŸ¬ìŠ¤í„° ë¼ë²¨ì„ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€
    df_analysis = df.copy()
    df_analysis['cluster'] = cluster_labels
    
    print(f"ë¶„ì„ ëŒ€ìƒ: {len(df_analysis)}ê°œ ê¸°ì‚¬")
    
    # ê° í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„
    unique_clusters = sorted(set(cluster_labels))
    
    for cluster_id in unique_clusters:
        if cluster_id == -1:
            print(f"\nğŸ“Œ ë…¸ì´ì¦ˆ í¬ì¸íŠ¸: {sum(cluster_labels == -1)}ê°œ")
            noise_data = df_analysis[df_analysis['cluster'] == -1]
            if 'category' in noise_data.columns:
                noise_categories = noise_data['category'].value_counts()
                print("   ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
                for cat, count in noise_categories.head(3).items():
                    print(f"     - {cat}: {count}ê°œ")
            continue
        
        cluster_data = df_analysis[df_analysis['cluster'] == cluster_id]
        print(f"\nğŸ“Œ í´ëŸ¬ìŠ¤í„° {cluster_id}: {len(cluster_data)}ê°œ ê¸°ì‚¬")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
        if 'category' in cluster_data.columns:
            category_dist = cluster_data['category'].value_counts()
            print("   ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
            for cat, count in category_dist.items():
                percentage = count / len(cluster_data) * 100
                print(f"     - {cat}: {count}ê°œ ({percentage:.1f}%)")
        
        # ëŒ€í‘œ ê¸°ì‚¬ ì œëª© (ì²˜ìŒ 5ê°œ)
        print("   ëŒ€í‘œ ê¸°ì‚¬:")
        for i, title in enumerate(cluster_data['title'].head(5)):
            print(f"     {i+1}. {title[:80]}{'...' if len(title) > 80 else ''}")
        
        # í‰ê·  ê¸°ì‚¬ ê¸¸ì´
        if 'contentLength' in cluster_data.columns:
            avg_length = cluster_data['contentLength'].mean()
            print(f"   í‰ê·  ê¸°ì‚¬ ê¸¸ì´: {avg_length:.0f}ì")
    
    return df_analysis

def save_results(df_with_clusters, cluster_labels, clusterer):
    """ê²°ê³¼ ì €ì¥"""
    
    print(f"\n=== 5ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ===")
    
    # í´ëŸ¬ìŠ¤í„°ë§ëœ ë°ì´í„°í”„ë ˆì„ ì €ì¥
    if df_with_clusters is not None:
        output_file = 'clustered_news_results.csv'
        df_with_clusters.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"âœ“ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼: {output_file}")
    
    # í´ëŸ¬ìŠ¤í„° ì •ë³´ JSONìœ¼ë¡œ ì €ì¥
    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
    n_noise = list(cluster_labels).count(-1)
    
    cluster_info = {
        'summary': {
            'total_articles': len(cluster_labels),
            'n_clusters': n_clusters,
            'n_clustered': len(cluster_labels) - n_noise,
            'n_noise': n_noise,
            'clustering_ratio': (len(cluster_labels) - n_noise) / len(cluster_labels)
        },
        'cluster_sizes': {},
        'parameters': {
            'algorithm': 'HDBSCAN',
            'min_cluster_size': 3,
            'min_samples': 2,
            'metric': 'euclidean',
            'cluster_selection_method': 'eom'
        },
        'cluster_labels': cluster_labels.tolist()
    }
    
    # ê° í´ëŸ¬ìŠ¤í„° í¬ê¸° ê³„ì‚°
    for label in cluster_labels:
        if label >= 0:
            cluster_info['cluster_sizes'][str(label)] = cluster_info['cluster_sizes'].get(str(label), 0) + 1
    
    with open('cluster_info.json', 'w', encoding='utf-8') as f:
        json.dump(cluster_info, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ í´ëŸ¬ìŠ¤í„° ì •ë³´: cluster_info.json")
    print(f"âœ“ ì‹œê°í™” ì´ë¯¸ì§€: news_clustering_tsne.png")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš€ ë‰´ìŠ¤ ê¸°ì‚¬ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘")
    print("=" * 50)
    
    try:
        # 1. Node.jsì—ì„œ ìƒì„±í•œ ì„ë² ë”© ë¡œë“œ
        df, embeddings = load_embeddings_from_nodejs()
        
        # ë°ì´í„° ê²€ì¦
        if embeddings.shape[0] == 0:
            print("âŒ ì„ë² ë”© ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return
        
        if embeddings.shape[0] < 3:
            print("âŒ í´ëŸ¬ìŠ¤í„°ë§í•˜ê¸°ì—ëŠ” ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. (ìµœì†Œ 3ê°œ í•„ìš”)")
            return
        
        # 2. HDBSCAN í´ëŸ¬ìŠ¤í„°ë§
        cluster_labels, clusterer = perform_hdbscan_clustering(
            embeddings, 
            min_cluster_size=max(3, embeddings.shape[0] // 20),  # ë°ì´í„° í¬ê¸°ì— ë”°ë¼ ë™ì  ì¡°ì •
            min_samples=2
        )
        
        # 3. ì‹œê°í™”
        embeddings_2d = visualize_clusters(embeddings, cluster_labels, method='tsne')
        
        # 4. í´ëŸ¬ìŠ¤í„° ë¶„ì„
        df_with_clusters = analyze_clusters(df, cluster_labels)
        
        # 5. ê²°ê³¼ ì €ì¥
        save_results(df_with_clusters, cluster_labels, clusterer)
        
        print("\n" + "=" * 50)
        print("ğŸ‰ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
        print("\nìƒì„±ëœ íŒŒì¼ë“¤:")
        print("  - clustered_news_results.csv (í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼)")
        print("  - cluster_info.json (í´ëŸ¬ìŠ¤í„° í†µê³„)")
        print("  - news_clustering_tsne.png (ì‹œê°í™”)")
        
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("  1. CSV íŒŒì¼ì„ ì—´ì–´ì„œ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ í™•ì¸")
        print("  2. PNG ì´ë¯¸ì§€ì—ì„œ í´ëŸ¬ìŠ¤í„° ë¶„í¬ ì‹œê°ì  í™•ì¸")
        print("  3. í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ê¸°ì‚¬ë“¤ì˜ ê³µí†µ ì£¼ì œ íŒŒì•…")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"\në¬¸ì œ í•´ê²° ë°©ë²•:")
        print(f"1. íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸:")
        print(f"   pip install pandas numpy scikit-learn matplotlib seaborn")
        print(f"   pip install hdbscan  # ë˜ëŠ” scikit-learn>=1.3.0")
        print(f"2. Node.js ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¨¼ì € ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸")
        print(f"3. íŒŒì¼ ê²½ë¡œì™€ ê¶Œí•œ í™•ì¸")
        
        import traceback
        print(f"\nìƒì„¸ ì˜¤ë¥˜:")
        traceback.print_exc()

if __name__ == "__main__":
    main()
