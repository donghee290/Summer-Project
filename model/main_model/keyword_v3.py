import pandas as pd
import numpy as np
import re
import json
from collections import Counter, defaultdict
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# í—ˆê¹…í˜ì´ìŠ¤ transformers ì‚¬ìš©ì„ ìœ„í•œ ì¶”ê°€ import
try:
    from transformers import AutoTokenizer, AutoModel
    import torch
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    print("Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")

def find_csv_file():
    """CSV íŒŒì¼ ì°¾ê¸°"""
    import os
    possible_paths = [
        'clustering_results_detailed.csv',
        './kosimcse_results/clustering_results_detailed.csv',
        '../clustering_results_detailed.csv'
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            return path
    return None

def create_comprehensive_stopwords():
    """í¬ê´„ì ì¸ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ìƒì„±"""
    # ê¸°ë³¸ ë¶ˆìš©ì–´
    basic_stopwords = {
        'ìˆë‹¤', 'ìˆëŠ”', 'ìˆì„', 'ìˆì–´', 'ìˆì—ˆë‹¤', 'ìˆìŠµë‹ˆë‹¤', 'ì—†ë‹¤', 'ì—†ëŠ”', 'ì—†ì„',
        'ë˜ë‹¤', 'ëœë‹¤', 'ë˜ëŠ”', 'ë ', 'ëë‹¤', 'ë©ë‹ˆë‹¤', 'í•˜ë‹¤', 'í•œë‹¤', 'í•˜ëŠ”', 'í• ', 'í–ˆë‹¤',
        'ì´ë‹¤', 'ì´ë©°', 'ì´ê³ ', 'ì´ë¼ê³ ', 'ë¼ê³ ', 'ë¼ë©°', 'ìœ¼ë¡œ', 'ë¡œ', 'ì—ì„œ', 'ì—ê²Œ', 'ì—', 'ì„', 'ë¥¼',
        'í†µí•´', 'ìœ„í•´', 'ìœ„í•œ', 'ëŒ€í•œ', 'ëŒ€í•´', 'í•¨ê»˜', 'ì´ë²ˆ', 'ì§€ë‚œ', 'ë‹¤ë¥¸', 'ê°™ì€', 'ë˜í•œ', 'í•œí¸',
        'ì´ì–´', 'ì´ì—', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ', 'ë•Œë¬¸ì—', 'ì´í›„', 'ì•ì„œ',
        'ì˜¤ëŠ”', 'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì–´ì œ', 'ìµœê·¼', 'ë‹¹ì‹œ', 'í˜„ì¬', 'ì´ë‚ ', 'ë‹¤ìŒ', 'ì§€ê¸ˆ',
        'ê²ƒìœ¼ë¡œ', 'ê²ƒì´ë‹¤', 'ê²ƒì„', 'ê²ƒì€', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë”', 'ê°€ì¥', 'ë§¤ìš°', 'ì •ë§', 'ì•„ì£¼'
    }
    
    # ë‰´ìŠ¤ íŠ¹í™” ë¶ˆìš©ì–´
    news_stopwords = {
        'ê¸°ì', 'ë‰´ìŠ¤', 'ì‚¬ì§„', 'ì œê³µ', 'ì—°í•©ë‰´ìŠ¤', 'ì—°í•©', 'ì•„ì‹œì•„', 'ì¡°ì„ ì¼ë³´', 'ì¤‘ì•™ì¼ë³´',
        'ëŒ“ê¸€', 'ë‹µê¸€', 'ì¶”ì²œ', 'ì¶”ì²œìˆ˜', 'ë…¸ì¶œ', 'ìë™ë“±ë¡ë°©ì§€', 'ë¡œê·¸ì¸', 'ë¹„ë°€ë²ˆí˜¸', 'íšŒì›',
        'ë‹¤ë¥¸ê¸°ì‚¬', 'ë³´ê¸°', 'ì‘ì„±ì', 'ì •ë ¬', 'ëª¨ìŒ', 'ê´‘ê³ ', 'í›„ì›', 'êµ¬ë…',
        'ê¸°ì‚¬', 'ë³´ë„', 'ë°œí‘œ', 'ë°œì–¸', 'ë§í–ˆë‹¤', 'ë°í˜”ë‹¤', 'ì „í–ˆë‹¤', 'ì„¤ëª…í–ˆë‹¤', 'ê°•ì¡°í–ˆë‹¤',
        'ì˜ˆì •ì´ë‹¤', 'ê³„íšì´ë‹¤', 'ì „ë§ì´ë‹¤', 'ì˜ˆìƒëœë‹¤', 'ê´€ì¸¡ëœë‹¤'
    }
    
    # ì¼ë°˜ì ì¸ í˜•ìš©ì‚¬/ë¶€ì‚¬
    descriptive_stopwords = {
        'í¬ë‹¤', 'ì‘ë‹¤', 'ì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤', 'ë§ë‹¤', 'ì ë‹¤', 'ë¹ ë¥´ë‹¤', 'ëŠë¦¬ë‹¤', 'ë†’ë‹¤', 'ë‚®ë‹¤',
        'ìƒˆë¡œìš´', 'ì˜¤ë˜ëœ', 'ì²˜ìŒ', 'ë§ˆì§€ë§‰', 'ë‹¤ì–‘í•œ', 'ì—¬ëŸ¬', 'ëª¨ë“ ', 'ì „ì²´', 'ì¼ë¶€', 'ê°ê°',
        'íŠ¹íˆ', 'ì£¼ë¡œ', 'ê±°ì˜', 'ì•½', 'ëŒ€ëµ', 'ì •ë„', 'ë§Œí¼', 'ì •ë§', 'ë§¤ìš°', 'ë„ˆë¬´', 'ì¡°ê¸ˆ'
    }
    
    return basic_stopwords | news_stopwords | descriptive_stopwords

def advanced_text_preprocessing(text, stopwords):
    """ê³ ê¸‰ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
    if pd.isna(text):
        return ""
    
    text = str(text)
    
    # 1. ê¸°ë³¸ ì •ë¦¬
    text = re.sub(r'[^\w\sê°€-í£]', ' ', text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r'\d+', ' ', text)  # ìˆ«ì ì œê±°
    text = re.sub(r'[a-zA-Z]+', ' ', text)  # ì˜ë¬¸ ì œê±°
    text = re.sub(r'\s+', ' ', text).strip()  # ê³µë°± ì •ë¦¬
    
    # 2. í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ ë° ê¸¸ì´ í•„í„°ë§
    words = re.findall(r'[ê°€-í£]{2,}', text)
    
    # 3. ë¶ˆìš©ì–´ ì œê±°
    words = [word for word in words if word not in stopwords]
    
    # 4. ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ë§Œ ì„ ë³„ (ë¹ˆë„ ê¸°ë°˜ì´ ì•„ë‹Œ ê¸¸ì´ì™€ íŒ¨í„´ ê¸°ë°˜)
    meaningful_words = []
    for word in words:
        # ë„ˆë¬´ ê¸´ ë‹¨ì–´ ì œì™¸ (ì˜¤íƒ€ë‚˜ ì˜ë¯¸ì—†ëŠ” ì¡°í•©ì¼ ê°€ëŠ¥ì„±)
        if len(word) > 8:
            continue
        # ë°˜ë³µ íŒ¨í„´ ì œì™¸ (ì˜ˆ: "í•˜í•˜í•˜", "ì•„ì•„ì•„")
        if len(set(word)) == 1:
            continue
        meaningful_words.append(word)
    
    return ' '.join(meaningful_words)

def extract_meaningful_keywords_with_context(df, category, stopwords):
    """ë¬¸ë§¥ì„ ê³ ë ¤í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    category_data = df[df['category'] == category].copy()
    
    # ì œëª©ê³¼ ë‚´ìš©ì„ ê²°í•©í•˜ë˜ ì œëª©ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬
    combined_texts = []
    for _, row in category_data.iterrows():
        title = str(row['title']) if pd.notna(row['title']) else ""
        content = str(row['content']) if pd.notna(row['content']) else ""
        
        # ì œëª©ì„ 3ë²ˆ ë°˜ë³µí•˜ì—¬ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        combined = f"{title} {title} {title} {content}"
        processed = advanced_text_preprocessing(combined, stopwords)
        combined_texts.append(processed)
    
    if not combined_texts or all(not text for text in combined_texts):
        return [], []
    
    # TF-IDFë¡œ ì¤‘ìš”í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
    try:
        vectorizer = TfidfVectorizer(
            max_features=50,
            min_df=3,
            max_df=0.7,
            ngram_range=(1, 3),  # 1-3gram ì‚¬ìš©
            token_pattern=r'[ê°€-í£]{2,}'
        )
        
        tfidf_matrix = vectorizer.fit_transform(combined_texts)
        feature_names = vectorizer.get_feature_names_out()
        
        # ì „ì²´ ë¬¸ì„œì—ì„œì˜ TF-IDF ì ìˆ˜ í•©ê³„ë¡œ í‚¤ì›Œë“œ ì¤‘ìš”ë„ ê³„ì‚°
        tfidf_scores = np.array(tfidf_matrix.sum(axis=0)).flatten()
        keyword_importance = [(feature_names[i], float(tfidf_scores[i])) for i in range(len(feature_names))]
        keyword_importance.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ í‚¤ì›Œë“œë“¤ê³¼ í•´ë‹¹ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê¸°ì‚¬ë“¤ ì°¾ê¸°
        top_keywords = [kw for kw, score in keyword_importance[:15] if score > 0.1]
        
        # í‚¤ì›Œë“œë³„ë¡œ ê´€ë ¨ ê¸°ì‚¬ ì°¾ê¸°
        keyword_articles = []
        for idx, text in enumerate(combined_texts):
            if not text:
                continue
                
            matching_keywords = [kw for kw in top_keywords if kw in text]
            if matching_keywords:
                row = category_data.iloc[idx]
                keyword_articles.append({
                    'index': int(idx),
                    'title': str(row['title'])[:100] + "..." if len(str(row['title'])) > 100 else str(row['title']),
                    'content': str(row['content'])[:200] + "..." if len(str(row['content'])) > 200 else str(row['content']),
                    'matching_keywords': matching_keywords,
                    'keyword_count': len(matching_keywords)
                })
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ìˆ˜ê°€ ë§ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        keyword_articles.sort(key=lambda x: x['keyword_count'], reverse=True)
        
        return top_keywords, keyword_articles
        
    except Exception as e:
        print(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return [], []

def perform_semantic_clustering_hf(texts, category, n_clusters=None):
    """Hugging Face ëª¨ë¸ì„ ì‚¬ìš©í•œ ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„°ë§"""
    if not HF_AVAILABLE or not texts:
        return None, None, []
    
    try:
        # í•œêµ­ì–´ BERT ëª¨ë¸ ì‚¬ìš©
        model_name = "klue/bert-base"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        
        # í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        embeddings = []
        valid_texts = []
        for i, text in enumerate(texts[:50]):  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ìƒìœ„ 50ê°œë§Œ
            if not text.strip():
                continue
                
            inputs = tokenizer(text[:512], return_tensors="pt", truncation=True, padding=True)
            with torch.no_grad():
                outputs = model(**inputs)
                # [CLS] í† í°ì˜ ì„ë² ë”© ì‚¬ìš©
                embedding = outputs.last_hidden_state[:, 0, :].numpy().flatten()
                embeddings.append(embedding)
                valid_texts.append(i)
        
        if len(embeddings) < 3:
            return None, None, []
        
        embeddings = np.array(embeddings)
        
        # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
        if n_clusters is None:
            n_clusters = min(5, max(2, len(embeddings) // 10))
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(embeddings)
        
        # ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°
        try:
            silhouette_avg = silhouette_score(embeddings, cluster_labels)
        except:
            silhouette_avg = 0
        
        return kmeans, cluster_labels, valid_texts, float(silhouette_avg)
        
    except Exception as e:
        print(f"Hugging Face ëª¨ë¸ ì‚¬ìš© ì¤‘ ì˜¤ë¥˜: {e}")
        return None, None, [], 0

def perform_traditional_clustering(texts, category, stopwords):
    """ì „í†µì ì¸ TF-IDF ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ (ê°œì„ ëœ ë²„ì „)"""
    if not texts:
        return None, None, None, None
    
    processed_texts = [advanced_text_preprocessing(text, stopwords) for text in texts]
    processed_texts = [text for text in processed_texts if text.strip()]
    
    if len(processed_texts) < 3:
        return None, None, None, None
    
    try:
        vectorizer = TfidfVectorizer(
            max_features=100,
            min_df=2,
            max_df=0.8,
            ngram_range=(1, 2),
            token_pattern=r'[ê°€-í£]{2,}'
        )
        
        tfidf_matrix = vectorizer.fit_transform(processed_texts)
        
        # í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
        n_clusters = min(5, max(2, len(processed_texts) // 8))
        
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(tfidf_matrix)
        
        # ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°
        try:
            silhouette_avg = silhouette_score(tfidf_matrix.toarray(), cluster_labels)
        except:
            silhouette_avg = 0
        
        return kmeans, cluster_labels, vectorizer.get_feature_names_out(), float(silhouette_avg)
        
    except Exception as e:
        print(f"ì „í†µì  í´ëŸ¬ìŠ¤í„°ë§ ì˜¤ë¥˜: {e}")
        return None, None, None, 0

def main():
    # CSV íŒŒì¼ ì°¾ê¸°
    csv_path = find_csv_file()
    if csv_path is None:
        print("CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤: {csv_path}")
    df = pd.read_csv(csv_path)
    
    stopwords = create_comprehensive_stopwords()
    
    # ê²°ê³¼ë¥¼ ì €ì¥í•  ì „ì²´ êµ¬ì¡°
    analysis_results = {
        "metadata": {
            "analysis_date": datetime.now().isoformat(),
            "source_file": csv_path,
            "total_articles": len(df),
            "categories": list(df['category'].unique()),
            "huggingface_available": HF_AVAILABLE
        },
        "categories": {}
    }
    
    print("=== ê°œì„ ëœ í‚¤ì›Œë“œ ë¶„ì„ ë° í´ëŸ¬ìŠ¤í„°ë§ ===\n")
    
    for category in df['category'].unique():
        print(f"{'='*50}")
        print(f"ì¹´í…Œê³ ë¦¬: {category.upper()}")
        print(f"{'='*50}")
        
        category_data = df[df['category'] == category].copy()
        article_count = len(category_data)
        print(f"ì´ ê¸°ì‚¬ ìˆ˜: {article_count}")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼ êµ¬ì¡° ì´ˆê¸°í™”
        category_result = {
            "article_count": article_count,
            "keywords": {
                "top_keywords": [],
                "representative_articles": []
            },
            "clustering": {
                "method_used": "",
                "silhouette_score": 0,
                "clusters": []
            }
        }
        
        # ê°œì„ ëœ í‚¤ì›Œë“œ ì¶”ì¶œ
        top_keywords, keyword_articles = extract_meaningful_keywords_with_context(df, category, stopwords)
        
        if top_keywords:
            print(f"\nì£¼ìš” í‚¤ì›Œë“œ ({len(top_keywords)}ê°œ):")
            category_result["keywords"]["top_keywords"] = top_keywords
            
            for i, keyword in enumerate(top_keywords, 1):
                print(f"  {i:2d}. {keyword}")
            
            print(f"\ní‚¤ì›Œë“œ ê¸°ë°˜ ëŒ€í‘œ ê¸°ì‚¬ (ìƒìœ„ 5ê°œ):")
            for i, article in enumerate(keyword_articles[:5], 1):
                print(f"\n[{i}] {article['title']}")
                print(f"    ë§¤ì¹­ í‚¤ì›Œë“œ ({article['keyword_count']}ê°œ): {', '.join(article['matching_keywords'][:5])}")
                if len(article['matching_keywords']) > 5:
                    print(f"    ... ë° {len(article['matching_keywords']) - 5}ê°œ ë”")
            
            # ìƒìœ„ 10ê°œ ê¸°ì‚¬ë¥¼ JSONì— ì €ì¥
            category_result["keywords"]["representative_articles"] = keyword_articles[:10]
        
        # ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„°ë§ ì‹œë„
        texts = category_data['content'].fillna('').tolist()
        
        print(f"\n--- ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ---")
        
        # Hugging Face ëª¨ë¸ ì‹œë„
        hf_result = perform_semantic_clustering_hf(texts, category)
        
        if hf_result[0] is not None:
            kmeans_hf, cluster_labels_hf, valid_indices, silhouette_score = hf_result
            print("âœ… Hugging Face BERT ëª¨ë¸ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì„±ê³µ")
            
            category_result["clustering"]["method_used"] = "huggingface_bert"
            category_result["clustering"]["silhouette_score"] = silhouette_score
            
            for cluster_id in range(len(np.unique(cluster_labels_hf))):
                cluster_indices = np.where(cluster_labels_hf == cluster_id)[0]
                # valid_indicesë¥¼ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ ë§¤í•‘
                original_indices = [valid_indices[i] for i in cluster_indices]
                cluster_docs = category_data.iloc[original_indices]
                
                print(f"\nğŸ” í´ëŸ¬ìŠ¤í„° {cluster_id + 1} ({len(cluster_indices)}ê°œ ê¸°ì‚¬)")
                
                cluster_info = {
                    "cluster_id": cluster_id + 1,
                    "article_count": len(cluster_indices),
                    "keywords": [],
                    "representative_articles": []
                }
                
                # í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ ê¸°ì‚¬
                print("ëŒ€í‘œ ê¸°ì‚¬:")
                for i, (_, row) in enumerate(cluster_docs.head(3).iterrows()):
                    title = str(row['title'])[:80] + "..." if len(str(row['title'])) > 80 else str(row['title'])
                    print(f"  â€¢ {title}")
                    
                    cluster_info["representative_articles"].append({
                        "title": str(row['title']),
                        "content": str(row['content'])[:200] + "..." if len(str(row['content'])) > 200 else str(row['content'])
                    })
                
                category_result["clustering"]["clusters"].append(cluster_info)
        else:
            # ì „í†µì  ë°©ë²•ìœ¼ë¡œ ëŒ€ì²´
            print("âš ï¸  Hugging Face ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€, ì „í†µì  ë°©ë²• ì‚¬ìš©")
            trad_result = perform_traditional_clustering(texts, category, stopwords)
            
            if trad_result[0] is not None:
                kmeans, cluster_labels, feature_names, silhouette_score = trad_result
                
                category_result["clustering"]["method_used"] = "traditional_tfidf"
                category_result["clustering"]["silhouette_score"] = silhouette_score
                
                for cluster_id in range(len(np.unique(cluster_labels))):
                    cluster_indices = np.where(cluster_labels == cluster_id)[0]
                    cluster_docs = category_data.iloc[cluster_indices]
                    
                    print(f"\nğŸ” í´ëŸ¬ìŠ¤í„° {cluster_id + 1} ({len(cluster_indices)}ê°œ ê¸°ì‚¬)")
                    
                    cluster_info = {
                        "cluster_id": cluster_id + 1,
                        "article_count": len(cluster_indices),
                        "keywords": [],
                        "representative_articles": []
                    }
                    
                    # í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œ
                    cluster_center = kmeans.cluster_centers_[cluster_id]
                    top_indices = cluster_center.argsort()[-5:][::-1]
                    cluster_keywords = [feature_names[i] for i in top_indices if cluster_center[i] > 0.1]
                    
                    if cluster_keywords:
                        print(f"ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(cluster_keywords)}")
                        cluster_info["keywords"] = cluster_keywords
                    
                    # ëŒ€í‘œ ê¸°ì‚¬
                    print("ëŒ€í‘œ ê¸°ì‚¬:")
                    for i, (_, row) in enumerate(cluster_docs.head(3).iterrows()):
                        title = str(row['title'])[:80] + "..." if len(str(row['title'])) > 80 else str(row['title'])
                        print(f"  â€¢ {title}")
                        
                        cluster_info["representative_articles"].append({
                            "title": str(row['title']),
                            "content": str(row['content'])[:200] + "..." if len(str(row['content'])) > 200 else str(row['content'])
                        })
                    
                    category_result["clustering"]["clusters"].append(cluster_info)
            else:
                print("âŒ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨")
                category_result["clustering"]["method_used"] = "failed"
        
        # ì¹´í…Œê³ ë¦¬ ê²°ê³¼ë¥¼ ì „ì²´ ê²°ê³¼ì— ì¶”ê°€
        analysis_results["categories"][category] = category_result
        
        print(f"\n{'-'*50}\n")
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    output_filename = f"keyword_clustering_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    try:
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ë¶„ì„ ê²°ê³¼ê°€ '{output_filename}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ìš”ì•½ ì •ë³´ ì¶œë ¥
        print("\n=== ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ ìš”ì•½ ===")
        print(f"ì „ì²´ ê¸°ì‚¬ ìˆ˜: {analysis_results['metadata']['total_articles']}")
        print(f"ë¶„ì„ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(analysis_results['categories'])}")
        
        for category, result in analysis_results["categories"].items():
            print(f"\n[{category}]")
            print(f"  - ê¸°ì‚¬ ìˆ˜: {result['article_count']}")
            print(f"  - í‚¤ì›Œë“œ ìˆ˜: {len(result['keywords']['top_keywords'])}")
            print(f"  - í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•: {result['clustering']['method_used']}")
            print(f"  - í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(result['clustering']['clusters'])}")
            if result['clustering']['silhouette_score'] > 0:
                print(f"  - ì‹¤ë£¨ì—£ ì ìˆ˜: {result['clustering']['silhouette_score']:.3f}")
        
    except Exception as e:
        print(f"âŒ JSON íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()

